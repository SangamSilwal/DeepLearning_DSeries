{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5OjXssTz9R+tJvhjJfvwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SangamSilwal/DeepLearning_DSeries/blob/main/T7_hyperparameter_Tuning_optuna.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "J2jYs3AZ093q"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for the GPU\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WiaCrlMz18Fp",
        "outputId": "1ac924e7-dd0a-4f0e-cde2-12c0a7811dbe"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/fmnist_small.csv')\n",
        "X = df.iloc[:,1:].values\n",
        "y = df.iloc[:,0].values\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=42)\n",
        "X_train = X_train/255.0\n",
        "X_test = X_test/255.0"
      ],
      "metadata": {
        "id": "3otpwRYN2HG7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Custom Dataset class using Dataset and Dataset loader for batch operation\n",
        "# we need to provide the model input in batches for applying BatchNormalization tecnique\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  def __init__(self,features,labels):\n",
        "    self.features = torch.tensor(features,dtype=torch.float32)\n",
        "    self.labels = torch.tensor(labels,dtype=torch.long)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.features)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index], self.labels[index]\n",
        "\n",
        "train_dataset = CustomDataset(X_train,y_train)\n",
        "test_dataset = CustomDataset(X_test,y_test)"
      ],
      "metadata": {
        "id": "sojgtiKQ2UxZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we will create our ANN Architecture according to the optuna for model hyperparameter tuning\n",
        "class ANN(nn.Module):\n",
        "\n",
        "  def __init__(self,input_dim,output_dim,num_hidden_layers,neurons_per_layer,dropout_rate):\n",
        "    super().__init__()\n",
        "    layers = []\n",
        "\n",
        "    for i in range(num_hidden_layers):\n",
        "      layers.append(nn.Linear(input_dim,neurons_per_layer))\n",
        "      layers.append(nn.BatchNorm1d(neurons_per_layer))\n",
        "      layers.append(nn.ReLU())\n",
        "      layers.append(nn.Dropout(dropout_rate))\n",
        "      input_dim = neurons_per_layer\n",
        "    layers.append(nn.Linear(neurons_per_layer,output_dim))\n",
        "    self.model = nn.Sequential(*layers)\n",
        "\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "IR7Iyn-j3nn7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.optim import optimizer\n",
        "#Objective Function for hyperparameter Tuning\n",
        "def objective(trial):\n",
        "  #Setting the hyperparameters\n",
        "  num_hidden_layers = trial.suggest_int(\"num_hidden_layers\",1,5)\n",
        "  neurons_per_layer = trial.suggest_int(\"neuron_per_layer\",8,128,step=8)\n",
        "  epochs = trial.suggest_int(\"epochs\",10,50,step=10)\n",
        "  learning_rate = trial.suggest_float(\"learning_rate\",1e-5,1e-1,log=True)\n",
        "  dropout_rate = trial.suggest_float(\"dropour_rate\",0.1,0.5,step=0.1)\n",
        "  batch_size = trial.suggest_categorical(\"batch_size\",[16,32,64])\n",
        "  optimizer_name = trial.suggest_categorical(\"optimizer\",['Adam','SGD','RMSprop'])\n",
        "  weight_decay = trial.suggest_float(\"weight_decay\",1e-5,1e-3,log=True)\n",
        "\n",
        "  #Train Loader\n",
        "  train_loader = DataLoader(train_dataset,batch_size=batch_size,shuffle=True,pin_memory=True)\n",
        "  test_loader = DataLoader(test_dataset,batch_size=batch_size,shuffle=False,pin_memory=True)\n",
        "\n",
        "  # Model initialization\n",
        "  input_dim = 784\n",
        "  output_dim = 10\n",
        "\n",
        "  model = ANN(input_dim,output_dim,num_hidden_layers,neurons_per_layer,dropout_rate)\n",
        "  model.to(device)\n",
        "\n",
        "  # optimizer Selection\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  if optimizer_name == \"Adam\":\n",
        "    optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  elif optimizer_name == \"SGD\":\n",
        "    optimizer = optim.SGD(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "  else:\n",
        "    optimizer = optim.RMSprop(model.parameters(),lr=learning_rate,weight_decay=weight_decay)\n",
        "\n",
        "  #training Loop\n",
        "  for epoch in range(epochs):\n",
        "    for batch_features,batch_labels in train_loader:\n",
        "      batch_features,batch_labels = batch_features.to(device),batch_labels.to(device)\n",
        "      outputs = model(batch_features)\n",
        "      loss = criterion(outputs,batch_labels)\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "  # Model Evaluation\n",
        "  model.eval()\n",
        "  total = 0\n",
        "  correct = 0\n",
        "  with torch.no_grad():\n",
        "    for batch_features, batch_labels in test_loader:\n",
        "      batch_features,batch_labels = batch_features.to(device),batch_labels.to(device)\n",
        "      outputs = model(batch_features)\n",
        "      _,predicted = torch.max(outputs,1)\n",
        "      total = total + batch_labels.shape[0]\n",
        "      correct = correct + (predicted == batch_labels).sum().item()\n",
        "    accuracy = correct/total\n",
        "\n",
        "  return accuracy\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xH32aFhk5O1I"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASOih3-g-Sap",
        "outputId": "dc6170fc-9476-4dde-ffcb-922899fe53f7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.6.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (1.17.2)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.10.1-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.44)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.3)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.15.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.12/dist-packages (from Mako->alembic>=1.5.0->optuna) (3.0.3)\n",
            "Downloading optuna-4.6.0-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.7/404.7 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.10.1-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, optuna\n",
            "Successfully installed colorlog-6.10.1 optuna-4.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna\n",
        "study = optuna.create_study(direction='maximize')\n",
        "study.optimize(objective,n_trials=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gu4cuGp8-WGy",
        "outputId": "04cd9fc0-9851-4955-b98a-9e38f89053ba"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-12-01 08:14:23,096] A new study created in memory with name: no-name-f9f671f9-37ca-4427-a8ad-f7e9e779944f\n",
            "[I 2025-12-01 08:14:32,485] Trial 0 finished with value: 0.69 and parameters: {'num_hidden_layers': 2, 'neuron_per_layer': 112, 'epochs': 10, 'learning_rate': 0.0003617367193490889, 'dropour_rate': 0.2, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 5.1415990645892665e-05}. Best is trial 0 with value: 0.69.\n",
            "[I 2025-12-01 08:14:36,959] Trial 1 finished with value: 0.8091666666666667 and parameters: {'num_hidden_layers': 1, 'neuron_per_layer': 16, 'epochs': 30, 'learning_rate': 0.0001098624247693269, 'dropour_rate': 0.2, 'batch_size': 64, 'optimizer': 'RMSprop', 'weight_decay': 3.9566676737632546e-05}. Best is trial 1 with value: 0.8091666666666667.\n",
            "[I 2025-12-01 08:14:56,068] Trial 2 finished with value: 0.8275 and parameters: {'num_hidden_layers': 5, 'neuron_per_layer': 96, 'epochs': 20, 'learning_rate': 0.05150433106691456, 'dropour_rate': 0.1, 'batch_size': 16, 'optimizer': 'SGD', 'weight_decay': 0.0007999964794932355}. Best is trial 2 with value: 0.8275.\n",
            "[I 2025-12-01 08:15:06,199] Trial 3 finished with value: 0.8258333333333333 and parameters: {'num_hidden_layers': 1, 'neuron_per_layer': 80, 'epochs': 40, 'learning_rate': 0.004517575840480288, 'dropour_rate': 0.2, 'batch_size': 32, 'optimizer': 'SGD', 'weight_decay': 0.0008357848714088888}. Best is trial 2 with value: 0.8275.\n",
            "[I 2025-12-01 08:15:12,040] Trial 4 finished with value: 0.8075 and parameters: {'num_hidden_layers': 1, 'neuron_per_layer': 128, 'epochs': 40, 'learning_rate': 0.00027396591242351715, 'dropour_rate': 0.1, 'batch_size': 64, 'optimizer': 'RMSprop', 'weight_decay': 0.0006223847693740185}. Best is trial 2 with value: 0.8275.\n",
            "[I 2025-12-01 08:15:51,491] Trial 5 finished with value: 0.83 and parameters: {'num_hidden_layers': 4, 'neuron_per_layer': 64, 'epochs': 40, 'learning_rate': 0.0017794322602305555, 'dropour_rate': 0.4, 'batch_size': 16, 'optimizer': 'Adam', 'weight_decay': 1.5392180358834937e-05}. Best is trial 5 with value: 0.83.\n",
            "[I 2025-12-01 08:15:53,367] Trial 6 finished with value: 0.7691666666666667 and parameters: {'num_hidden_layers': 2, 'neuron_per_layer': 128, 'epochs': 10, 'learning_rate': 0.03374542748398246, 'dropour_rate': 0.4, 'batch_size': 64, 'optimizer': 'Adam', 'weight_decay': 0.00014082090934225324}. Best is trial 5 with value: 0.83.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_value"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pIsicLE-iJu",
        "outputId": "7ca44c29-2eb8-4b2f-d7ec-b8b3afac405e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.83"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "study.best_params"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6wL33TK-8je",
        "outputId": "5ae84df0-33d0-466a-92ce-0de6fb627210"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'num_hidden_layers': 4,\n",
              " 'neuron_per_layer': 64,\n",
              " 'epochs': 40,\n",
              " 'learning_rate': 0.0017794322602305555,\n",
              " 'dropour_rate': 0.4,\n",
              " 'batch_size': 16,\n",
              " 'optimizer': 'Adam',\n",
              " 'weight_decay': 1.5392180358834937e-05}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2H1NSM2b--ow"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}